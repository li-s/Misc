#CNN

## Brief overview

### First layers

#### Convolution

1. Runs x number of filters(feature identifiers) across whole image. Each filter represents a specific shape that is to be identified from the image(ie lines, curves on shallow layers, or eyes, noses on deeper layers). Outputs the similarity of the features as numbers as the next layer.
2. Stride determines the distance filter moves, resulting in different output sizes.
3. Padding fills the border of the output with predetermined units(eg 1), so keep input and output size the same.

#### Activation

ReLU (Rectified Linear Units) applies max(0, x) on each input -> changes each negative element to 0. This is to introduce non-linearity to the model

#### Pooling

Downsamples the data. For maxpooling, a filter of x size and x stride goes around the data and outputs the largest number of each subregion it passes through.
This reduces computation cost, and overfitting.

#### Dropout

Sets random activations to 0 in a forward pass. This helps the model train to identify images even when some information is missing.

### Training

1. Passses image through network, and obtain result.(forward pass)
2. Identify loss as a derivative.(loss function)
3. Identify which weights contribute the most to the loss.(backward pass)
4. Update weights.(parameter update)

# Something
## Perceptron neurons
- Input: 0, 1
-> Multiply weights, determine threashold(-b). (step function)
- Output: 0, 1

## Sgimoid neurons
- Input: 0 - 1
-> Apply sigmoid function(1/(1+exp(-Z))), where Z == W . X + b (W = weight, X  = input)
- Output: 0 - 1

## Why use loss function(quadratic cost) and not number of correct answers?
-> When weights change, correct answers may not increase, instead, the probability of getting the correct answer increase. If we use smooth functions, it is much easier to see how much a change in weights affect the output.

## RNN vs CNN
- CNN is a feed forward network -> input to output is one directional.
- RNN is a recurrent network -> deeper layers can output to shallower layers.

## How to find minimum point of loss
Let C denote quadratic cost function, V denote the total variables(V1, V2, V3, ..., Vm) within the model and η the learning rate. Since finding the individual derivative of C for each value of Vx is slow for large number of V, we denote ∇C as the vector of partial derivatives below.
- ∇C ≡ (∂C/∂V1, …, ∂C/∂Vm)T
Since ΔC is equal to the equation below,
- ΔC ≈ ∂C/∂v1 ⋅ Δv1 + ∂C/∂v2 ⋅ Δv2
we can rewrite the equation as:
- ΔC ≈ ∇C ⋅ ΔV
Since we are trying to minimise C, by setting ΔV as the equation below, we can ensure ΔC is always negative.
- Δv = −η∇C
